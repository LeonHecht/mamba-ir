{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Legal documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon/tesis/spanish-legal-ir/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the special token\n",
    "tokenizer.add_special_tokens({'sep_token': '<|sep|>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|sep|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3/18 [00:05<00:29,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping legislacion_boe_es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 7/18 [00:06<00:06,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 13/18 [00:15<00:06,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping corpus_tmp.txt\n",
      "Skipping corpus.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 16/18 [00:15<00:01,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping multiun_es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:18<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "corpus_text = \"\"\n",
    "\n",
    "with open(\"dataset/corpus.txt\", \"w\") as out_file:\n",
    "    for folder in tqdm(os.listdir(\"dataset\")):\n",
    "        if folder == \"corpus.zip\" or folder == \"corpus_tmp.txt\" or folder == \"corpus.txt\" \\\n",
    "            or folder == \"legislacion_boe_es\" or folder == \"multiun_es\":\n",
    "            print(f\"Skipping {folder}\")\n",
    "        else:\n",
    "            txt_path = os.path.join(\"dataset\", folder, \"output.txt\")\n",
    "            with open(txt_path, 'r') as f:\n",
    "                txt_content = f.read()\n",
    "                out_file.write(txt_content)\n",
    "                # add sep token to indicate end of document\n",
    "                out_file.write(tokenizer.sep_token)\n",
    "                out_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def count_lines(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "# Function to tokenize in batches\n",
    "def count_tokens(file_path, num_lines, batch_size=1000):\n",
    "    total_tokens = 0\n",
    "    batch = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, total=num_lines, desc=\"Processing lines\"):\n",
    "            batch.append(line.strip())\n",
    "            if len(batch) == batch_size:\n",
    "                # Tokenize the batch and count tokens\n",
    "                encoded_batch = tokenizer(batch, truncation=False, add_special_tokens=False)\n",
    "                total_tokens += sum(len(tokens) for tokens in encoded_batch[\"input_ids\"])\n",
    "                batch = []\n",
    "\n",
    "        # Process remaining lines in the batch\n",
    "        if batch:\n",
    "            encoded_batch = tokenizer(batch, truncation=False, add_special_tokens=False)\n",
    "            total_tokens += sum(len(tokens) for tokens in encoded_batch[\"input_ids\"])\n",
    "    \n",
    "    return total_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get number of tokens of each legal document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset/doctrina_fiscalia_es/output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines:   0%|          | 0/74544 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|██████████| 74544/74544 [00:03<00:00, 20491.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in doctrina_fiscalia_es: 4812960\n",
      "Processing dataset/dictamenes_consejo_estado_es/output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|██████████| 5359576/5359576 [03:28<00:00, 25682.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in dictamenes_consejo_estado_es: 245762157\n",
      "Processing dataset/jrc_acquis_es/output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|██████████| 6006032/6006032 [02:38<00:00, 37875.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in jrc_acquis_es: 109787641\n",
      "Processing dataset/legislacion_boe_es/output.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "with open(\"number_of_tokens.txt\", \"w\") as out_file:\n",
    "    for folder in os.listdir(\"dataset\"):\n",
    "        if folder.startswith(\"corpus\"):\n",
    "            print(f\"Skipping {folder}\")\n",
    "        else:\n",
    "            file_path = \"dataset/\" + folder + \"/output.txt\"\n",
    "            print(f\"Processing {file_path}\")\n",
    "            num_lines = count_lines(file_path)\n",
    "            total_tokens = count_tokens(file_path, num_lines)\n",
    "            print(f\"Tokens in {folder}: {total_tokens}\")\n",
    "            out_file.write(f\"{folder}: {total_tokens}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get goal number of tokens of each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 2717942069\n",
      "Number of documents: 15\n",
      "{'spanish_constitution_eu-ca-es': 34589, 'eurlex_es': 140790, 'doctrina_fiscalia_es': 4812960, 'un_opus_es': 6705807, 'abogacia_estado_boe_es': 11242268, 'codigos_universitarios_es': 22890142, 'codigos_electronicos_es': 23896311, 'patentes_medicas': 29574697, 'europarl_es': 97908059, 'jrc_acquis_es': 109787641, 'consultas_tributarias_es': 139173424, 'dictamenes_consejo_estado_es': 245762157, 'dogc_ca-es': 253103213, 'multiun_es': 644140252, 'legislacion_boe_es': 1128769759}\n",
      "Adding spanish_constitution_eu-ca-es to corpus\n",
      "Adding eurlex_es to corpus\n",
      "Adding doctrina_fiscalia_es to corpus\n",
      "Adding un_opus_es to corpus\n",
      "Adding abogacia_estado_boe_es to corpus\n",
      "Adding codigos_universitarios_es to corpus\n",
      "Adding codigos_electronicos_es to corpus\n",
      "{'spanish_constitution_eu-ca-es': 34589, 'eurlex_es': 140790, 'doctrina_fiscalia_es': 4812960, 'un_opus_es': 6705807, 'abogacia_estado_boe_es': 11242268, 'codigos_universitarios_es': 22890142, 'codigos_electronicos_es': 23896311}\n",
      "Lenght of corpus_meta_dict: 7\n",
      "{'spanish_constitution_eu-ca-es': 34589, 'eurlex_es': 140790, 'doctrina_fiscalia_es': 4812960, 'un_opus_es': 6705807, 'abogacia_estado_boe_es': 11242268, 'codigos_universitarios_es': 22890142, 'codigos_electronicos_es': 23896311, 'patentes_medicas': 28784641, 'europarl_es': 28784641, 'jrc_acquis_es': 28784641, 'consultas_tributarias_es': 28784642, 'dictamenes_consejo_estado_es': 28784642, 'dogc_ca-es': 28784642, 'multiun_es': 28784642, 'legislacion_boe_es': 28784642}\n",
      "Total tokens: 300000000\n"
     ]
    }
   ],
   "source": [
    "number_of_tokens = {\n",
    "    \"doctrina_fiscalia_es\": 4812960,\n",
    "    \"dictamenes_consejo_estado_es\": 245762157,\n",
    "    \"jrc_acquis_es\": 109787641,\n",
    "    \"legislacion_boe_es\": 1128769759,\n",
    "    \"codigos_universitarios_es\": 22890142,\n",
    "    \"codigos_electronicos_es\": 23896311,\n",
    "    \"un_opus_es\": 6705807,\n",
    "    \"consultas_tributarias_es\": 139173424,\n",
    "    \"patentes_medicas\": 29574697,\n",
    "    \"dogc_ca-es\": 253103213,\n",
    "    \"eurlex_es\": 140790,\n",
    "    \"spanish_constitution_eu-ca-es\": 34589,\n",
    "    \"abogacia_estado_boe_es\": 11242268,\n",
    "    \"multiun_es\": 644140252,\n",
    "    \"europarl_es\": 97908059,\n",
    "}\n",
    "\n",
    "total_tokens = sum(number_of_tokens.values())\n",
    "print(f\"Total tokens: {total_tokens}\")      # 2.7B\n",
    "\n",
    "max_tokens = 300_000_000        # 300M\n",
    "\n",
    "num_documents = len(number_of_tokens.keys())\n",
    "print(f\"Number of documents: {num_documents}\")     # 15\n",
    "\n",
    "# sort number_of_tokens by value\n",
    "number_of_tokens = dict(sorted(number_of_tokens.items(), key=lambda x: x[1]))\n",
    "\n",
    "print(number_of_tokens)\n",
    "\n",
    "corpus_meta_dict = {}\n",
    "\n",
    "# iterate through dict\n",
    "for key, value in list(number_of_tokens.items()):\n",
    "    if value < (max_tokens // num_documents):\n",
    "        corpus_meta_dict[key] = value\n",
    "        max_tokens -= value\n",
    "        num_documents -= 1\n",
    "        print(f\"Adding {key} to corpus\")\n",
    "        # remove key from number_of_tokens\n",
    "        del number_of_tokens[key]\n",
    "\n",
    "print(corpus_meta_dict)\n",
    "print(f\"Lenght of corpus_meta_dict: {len(corpus_meta_dict)}\")\n",
    "\n",
    "for key, value in list(number_of_tokens.items()):\n",
    "    tokens_allowed = max_tokens // num_documents\n",
    "    # cut the document to only contain the allowed tokens\n",
    "    corpus_meta_dict[key] = tokens_allowed\n",
    "    max_tokens -= tokens_allowed\n",
    "    num_documents -= 1\n",
    "\n",
    "print(corpus_meta_dict)\n",
    "total_tokens = sum(corpus_meta_dict.values())\n",
    "print(f\"Total tokens: {total_tokens}\")      # 300M\n",
    "\n",
    "# Save the meta data\n",
    "with open(\"corpus_meta.txt\", \"w\") as out_file:\n",
    "    for key, value in corpus_meta_dict.items():\n",
    "        out_file.write(f\"{key}: {value}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def chunk_and_tokenize_text(file_path, tokenizer, max_length=2048, overlap=0, buffer_size=10_000_000, output_path=\"pretokenized_data_300M.pkl\"):\n",
    "    \"\"\"\n",
    "    Split a large text file into tokenized chunks of input_ids and attention_masks based on max_length tokens with optional overlap,\n",
    "    and save them to disk.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the large text file.\n",
    "        tokenizer: Tokenizer for encoding the text.\n",
    "        max_length (int): Maximum number of tokens per chunk.\n",
    "        overlap (int): Number of tokens to overlap between chunks.\n",
    "        buffer_size (int): Number of characters to read from the file at a time.\n",
    "        output_path (str): Path to save the pretokenized dataset.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    tokenized_data = []\n",
    "    leftover_tokens = []\n",
    "\n",
    "    # Get file size to set up the progress bar\n",
    "    file_size = os.path.getsize(file_path)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f, tqdm(total=file_size, unit=\"B\", unit_scale=True, desc=\"Processing file\") as pbar:\n",
    "        while True:\n",
    "            # Read a portion of the text file\n",
    "            text_chunk = f.read(buffer_size)\n",
    "            if not text_chunk:\n",
    "                break\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(len(text_chunk.encode(\"utf-8\")))\n",
    "\n",
    "            # Combine leftover tokens with the new chunk\n",
    "            if leftover_tokens:\n",
    "                text_chunk = tokenizer.decode(leftover_tokens, skip_special_tokens=True) + text_chunk\n",
    "\n",
    "            # Tokenize the combined text\n",
    "            tokens = tokenizer.encode(text_chunk, truncation=False)\n",
    "\n",
    "            # Split tokens into chunks and save tokenized data\n",
    "            for i in range(0, len(tokens), max_length - overlap):\n",
    "                token_chunk = tokens[i:i + max_length]\n",
    "                if len(token_chunk) < max_length:\n",
    "                    # Save leftover tokens for the next iteration\n",
    "                    leftover_tokens = token_chunk\n",
    "                    break\n",
    "\n",
    "                # Add token chunk with padding to match max_length\n",
    "                padded_chunk = tokenizer.pad(\n",
    "                    {\"input_ids\": [token_chunk]},\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "                # Save tokenized chunk\n",
    "                tokenized_data.append({\n",
    "                    \"input_ids\": padded_chunk[\"input_ids\"].squeeze(0).tolist(),\n",
    "                    \"attention_mask\": padded_chunk[\"attention_mask\"].squeeze(0).tolist()\n",
    "                })\n",
    "            else:\n",
    "                leftover_tokens = []\n",
    "\n",
    "    # Handle any leftover tokens\n",
    "    if leftover_tokens:\n",
    "        padded_chunk = tokenizer.pad(\n",
    "            {\"input_ids\": [leftover_tokens]},\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        tokenized_data.append({\n",
    "            \"input_ids\": padded_chunk[\"input_ids\"].squeeze(0).tolist(),\n",
    "            \"attention_mask\": padded_chunk[\"attention_mask\"].squeeze(0).tolist()\n",
    "        })\n",
    "\n",
    "    # Save the tokenized data to disk\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        pickle.dump(tokenized_data, f)\n",
    "\n",
    "    print(f\"Tokenized data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def chunk_and_tokenize_text_up_to(file_path, tokenizer, max_length=2048, overlap=0, buffer_size=10_000_000, max_tokens=300_000_000, output_path=\"pretokenized_data_300M.pkl\"):\n",
    "    \"\"\"\n",
    "    Split a large text file into tokenized chunks of input_ids and attention_masks based on max_length tokens with optional overlap,\n",
    "    and save them to disk. Stop when the total number of tokens reaches max_tokens.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the large text file.\n",
    "        tokenizer: Tokenizer for encoding the text.\n",
    "        max_length (int): Maximum number of tokens per chunk.\n",
    "        overlap (int): Number of tokens to overlap between chunks.\n",
    "        buffer_size (int): Number of characters to read from the file at a time.\n",
    "        max_tokens (int): Maximum number of tokens to process.\n",
    "        output_path (str): Path to save the pretokenized dataset.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    tokenized_data = []\n",
    "    leftover_tokens = []\n",
    "    total_tokens = 0  # Keep track of the total number of tokens processed\n",
    "\n",
    "    # Get file size to set up the progress bar\n",
    "    file_size = os.path.getsize(file_path)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f, tqdm(total=file_size, unit=\"B\", unit_scale=True, desc=\"Processing file\") as pbar:\n",
    "        while total_tokens < max_tokens:  # Stop if max_tokens is reached\n",
    "            # Read a portion of the text file\n",
    "            text_chunk = f.read(buffer_size)\n",
    "            if not text_chunk:\n",
    "                print(\"End of file reached\")\n",
    "                break\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(len(text_chunk.encode(\"utf-8\")))\n",
    "\n",
    "            # Combine leftover tokens with the new chunk\n",
    "            if leftover_tokens:\n",
    "                text_chunk = tokenizer.decode(leftover_tokens, skip_special_tokens=True) + text_chunk\n",
    "\n",
    "            # Tokenize the combined text\n",
    "            tokens = tokenizer.encode(text_chunk, truncation=False)\n",
    "\n",
    "            # Split tokens into chunks and save tokenized data\n",
    "            for i in range(0, len(tokens), max_length - overlap):\n",
    "                token_chunk = tokens[i:i + max_length]\n",
    "                if len(token_chunk) < max_length:\n",
    "                    # Save leftover tokens for the next iteration if we can't fill the max_length\n",
    "                    # This happens when len(tokens) is not a perfect multiple of max_length - overlap\n",
    "                    leftover_tokens = token_chunk\n",
    "                    break\n",
    "\n",
    "                # Add token chunk with padding to match max_length\n",
    "                padded_chunk = tokenizer.pad(\n",
    "                    {\"input_ids\": [token_chunk]},\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "                # Save tokenized chunk\n",
    "                tokenized_data.append({\n",
    "                    \"input_ids\": padded_chunk[\"input_ids\"].squeeze(0).tolist(),\n",
    "                    \"attention_mask\": padded_chunk[\"attention_mask\"].squeeze(0).tolist()\n",
    "                })\n",
    "                total_tokens += len(token_chunk)  # Update the total token count\n",
    "\n",
    "                # Check if max_tokens is reached\n",
    "                if total_tokens >= max_tokens:\n",
    "                    print(f\"Reached max_tokens limit: {max_tokens}\")\n",
    "                    break\n",
    "            else:\n",
    "                # we enter into else if the for loop was not broken (with \"break\")\n",
    "                # And terminates naturally\n",
    "                leftover_tokens = []\n",
    "                continue  # Continue outer loop if inner loop wasn't broken\n",
    "            if total_tokens >= max_tokens:\n",
    "                break  # Break outer loop if inner loop was broken\n",
    "\n",
    "    # Save the tokenized data to disk\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        pickle.dump(tokenized_data, f)\n",
    "\n",
    "    print(f\"Tokenized data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chunk_text(file_path, tokenizer, max_length=2048, overlap=0, buffer_size=10_000_000):\n",
    "#     \"\"\"\n",
    "#     Split a large text file into chunks of text based on max_length tokens with optional overlap.\n",
    "\n",
    "#     Args:\n",
    "#         file_path (str): Path to the large text file.\n",
    "#         tokenizer: Tokenizer for encoding the text.\n",
    "#         max_length (int): Maximum number of tokens per chunk.\n",
    "#         overlap (int): Number of tokens to overlap between chunks.\n",
    "#         buffer_size (int): Number of characters to read from the file at a time.\n",
    "\n",
    "#     Returns:\n",
    "#         list: List of text chunks corresponding to the token limits.\n",
    "#     \"\"\"\n",
    "#     text_chunks = []\n",
    "#     leftover_tokens = []\n",
    "\n",
    "#     # Get file size to set up the progress bar\n",
    "#     file_size = os.path.getsize(file_path)\n",
    "#     processed_size = 0\n",
    "\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f, tqdm(total=file_size, unit=\"B\", unit_scale=True, desc=\"Processing file\") as pbar:\n",
    "#         while True:\n",
    "#             # Read a portion of the text file\n",
    "#             text_chunk = f.read(buffer_size)\n",
    "#             if not text_chunk:\n",
    "#                 break\n",
    "\n",
    "#             processed_size += len(text_chunk.encode(\"utf-8\"))\n",
    "#             pbar.update(len(text_chunk.encode(\"utf-8\")))\n",
    "\n",
    "#             # Combine leftover tokens with the new chunk\n",
    "#             if leftover_tokens:\n",
    "#                 text_chunk = tokenizer.decode(leftover_tokens) + text_chunk\n",
    "\n",
    "#             # Tokenize the combined text\n",
    "#             tokens = tokenizer.encode(text_chunk, truncation=False)\n",
    "\n",
    "#             # Split tokens into chunks and decode back to text\n",
    "#             for i in range(0, len(tokens), max_length - overlap):\n",
    "#                 token_chunk = tokens[i:i + max_length]\n",
    "#                 if len(token_chunk) < max_length:\n",
    "#                     # Save leftover tokens for the next iteration\n",
    "#                     leftover_tokens = token_chunk\n",
    "#                     break\n",
    "\n",
    "#                 # Decode token chunk back to text and save\n",
    "#                 text_chunks.append(tokenizer.decode(token_chunk))\n",
    "#             else:\n",
    "#                 leftover_tokens = []\n",
    "\n",
    "#     # Add the remaining tokens as the last text chunk\n",
    "#     if leftover_tokens:\n",
    "#         text_chunks.append(tokenizer.decode(leftover_tokens))\n",
    "\n",
    "#     return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file:   0%|          | 0.00/1.10G [00:00<?, ?B/s]You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Processing file: 100%|██████████| 1.10G/1.10G [17:19<00:00, 1.06MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data saved to pretokenized_data_300M.pkl\n"
     ]
    }
   ],
   "source": [
    "# File path to the large text\n",
    "file_path = \"dataset/corpus_approx_300M.txt\"\n",
    "\n",
    "# Preprocess text into token chunks\n",
    "# token_chunks = chunk_text(file_path, tokenizer, max_length=2048, overlap=256)\n",
    "token_chunks = chunk_and_tokenize_text(file_path, tokenizer, max_length=2048, overlap=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping corpus_half_approx_500M_chunks.pkl\n",
      "Skipping corpus_half_approx_500M.txt\n",
      "Skipping corpus_approx_300M.txt\n",
      "Skipping corpus_tmp.txt\n",
      "Skipping corpus_945M_tokens.txt\n",
      "Tokenizing doctrina_fiscalia_es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file:   0%|          | 0.00/17.0M [00:00<?, ?B/s]You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Processing file: 100%|██████████| 17.0M/17.0M [00:15<00:00, 1.07MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached max_tokens limit: 4812960\n",
      "Tokenized data saved to dataset/doctrina_fiscalia_es/output_tokenized.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dictamenes_consejo_estado_es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file:  12%|█▏        | 102M/867M [01:28<11:09, 1.14MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached max_tokens limit: 28784642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data saved to dataset/dictamenes_consejo_estado_es/output_tokenized.pkl\n",
      "Tokenizing jrc_acquis_es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file:  27%|██▋       | 102M/372M [01:34<04:09, 1.08MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached max_tokens limit: 28784641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data saved to dataset/jrc_acquis_es/output_tokenized.pkl\n",
      "Tokenizing legislacion_boe_es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file:   2%|▏         | 91.2M/3.77G [01:27<59:03, 1.04MB/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached max_tokens limit: 28784642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data saved to dataset/legislacion_boe_es/output_tokenized.pkl\n",
      "Tokenizing codigos_universitarios_es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file: 100%|██████████| 79.6M/79.6M [01:10<00:00, 1.13MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached max_tokens limit: 22890142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data saved to dataset/codigos_universitarios_es/output_tokenized.pkl\n",
      "Tokenizing codigos_electronicos_es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file:  99%|█████████▊| 81.7M/82.8M [01:15<00:01, 1.08MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached max_tokens limit: 23896311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data saved to dataset/codigos_electronicos_es/output_tokenized.pkl\n",
      "Tokenizing un_opus_es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file: 100%|██████████| 23.5M/23.5M [00:21<00:00, 1.11MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached max_tokens limit: 6705807\n",
      "Tokenized data saved to dataset/un_opus_es/output_tokenized.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing consultas_tributarias_es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file:  20%|██        | 102M/497M [01:35<06:10, 1.07MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached max_tokens limit: 28784642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data saved to dataset/consultas_tributarias_es/output_tokenized.pkl\n",
      "Tokenizing patentes_medicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file: 100%|██████████| 87.1M/87.1M [01:34<00:00, 921kB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached max_tokens limit: 28784641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data saved to dataset/patentes_medicas/output_tokenized.pkl\n",
      "Tokenizing dogc_ca-es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file:  11%|█         | 91.8M/865M [01:36<13:33, 949kB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached max_tokens limit: 28784642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data saved to dataset/dogc_ca-es/output_tokenized.pkl\n",
      "Tokenizing eurlex_es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file: 100%|██████████| 490k/490k [00:00<00:00, 1.16MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached max_tokens limit: 140790\n",
      "Tokenized data saved to dataset/eurlex_es/output_tokenized.pkl\n",
      "Tokenizing spanish_constitution_eu-ca-es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file: 100%|██████████| 123k/123k [00:00<00:00, 1.20MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached max_tokens limit: 34589\n",
      "Tokenized data saved to dataset/spanish_constitution_eu-ca-es/output_tokenized.pkl\n",
      "Tokenizing abogacia_estado_boe_es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file: 100%|██████████| 38.5M/38.5M [00:38<00:00, 991kB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached max_tokens limit: 11242268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data saved to dataset/abogacia_estado_boe_es/output_tokenized.pkl\n",
      "Tokenizing multiun_es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file:   4%|▍         | 102M/2.30G [01:50<39:52, 919kB/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached max_tokens limit: 28784642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data saved to dataset/multiun_es/output_tokenized.pkl\n",
      "Tokenizing europarl_es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file:  29%|██▊       | 102M/355M [01:50<04:34, 922kB/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached max_tokens limit: 28784641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data saved to dataset/europarl_es/output_tokenized.pkl\n",
      "Final tokenized corpus saved with 146496 chunks.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "documents = []\n",
    "doc_name = []\n",
    "\n",
    "for folder in os.listdir(\"dataset\"):\n",
    "    if folder.startswith(\"corpus\"):\n",
    "        print(f\"Skipping {folder}\")\n",
    "    else:\n",
    "        file_path = \"dataset/\" + folder + \"/output.txt\"\n",
    "        documents.append(file_path)\n",
    "        doc_name.append(folder)\n",
    "\n",
    "tokenized_corpus = []\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    # Use the chunk_and_tokenize_text function to tokenize the document\n",
    "    output_path = f\"{doc.split('.')[0]}_tokenized.pkl\"\n",
    "    print(f\"Tokenizing {doc_name[i]}\")\n",
    "    max_tokens = corpus_meta_dict[doc_name[i]]\n",
    "    chunk_and_tokenize_text_up_to(doc, tokenizer, max_tokens=max_tokens, output_path=output_path)\n",
    "    \n",
    "    # Load the tokenized chunks\n",
    "    with open(output_path, \"rb\") as f:\n",
    "        tokenized_data = pickle.load(f)\n",
    "    \n",
    "    tokenized_corpus.extend(tokenized_data)\n",
    "\n",
    "# Save the final corpus\n",
    "with open(\"tokenized_diverse_corpus_300M.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenized_corpus, f)\n",
    "print(f\"Final tokenized corpus saved with {len(tokenized_corpus)} chunks.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
